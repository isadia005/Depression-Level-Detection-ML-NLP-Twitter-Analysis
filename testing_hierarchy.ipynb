{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uUGEV_C0I6zp","outputId":"dbcf3adc-74df-4310-ce3f-2d8bed26462b","executionInfo":{"status":"ok","timestamp":1685799955290,"user_tz":420,"elapsed":16,"user":{"displayName":"Sadia Iqbal","userId":"13577405437173025394"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n","[nltk_data]   Package vader_lexicon is already up-to-date!\n"]}],"source":["#Libraries\n","#...........................................snscraper...............................\n","import pandas as pd\n","import numpy as np\n","#...........................................preprocessing............................\n","import nltk\n","from nltk.corpus import stopwords\n","from textblob import TextBlob\n","from textblob import Word\n","\n","nltk.download(\"stopwords\")\n","nltk.download(\"punkt\")\n","nltk.download(\"wordnet\")\n","nltk.download(\"omw-1.4\")\n","#..........................................Polarization.............................\n","import nltk\n","nltk.downloader.download('vader_lexicon')\n","from nltk.sentiment import SentimentIntensityAnalyzer\n","sia = SentimentIntensityAnalyzer()\n","\n","#..........................................Saved k-mean model....................\n","import pickle\n","from collections import Counter"]},{"cell_type":"code","source":["#İmport dataset\n","df = pd.read_csv(\"/content/testing_dataset.csv\")\n","df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ltM7KloTP1Mc","outputId":"3bb9d34b-42ea-49bf-ede6-a356003921a5","executionInfo":{"status":"ok","timestamp":1685799956627,"user_tz":420,"elapsed":46,"user":{"displayName":"Sadia Iqbal","userId":"13577405437173025394"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["         username                                               text\n","0        @isadia_                        Just had the best day ever!\n","1        @isadia_                                Feeling down today.\n","2        @isadia_                        So excited for the weekend!\n","3        @isadia_             Can't believe how much I accomplished!\n","4        @isadia_                             This weather is awful.\n","5        @isadia_                              Feeling stressed out.\n","6        @isadia_                       Enjoying a relaxing evening.\n","7   @mishi_ali209                     Had a great time with friends.\n","8   @mishi_ali209                            Feeling sad and lonely.\n","9   @mishi_ali209                                Feeling down today.\n","10  @mishi_ali209                                 Can't stop crying.\n","11  @mishi_ali209                         So happy to see my family.\n","12  @mishi_ali209           I feel like everything is falling apart.\n","13  @mishi_ali209                   Enjoyed a nice walk in the park.\n","14  @mishi_ali209                              Feeling disappointed.\n","15  @mishi_ali209                    Looking forward to better days.\n","16  @mishi_ali209                     Feeling exhausted and drained.\n","17  @mishi_ali209                           Missing someone special.\n","18  @mishi_ali209                          Just had a great workout.\n","19         @w_n_w                                Had an amazing day!\n","20         @w_n_w                    Feeling positive and motivated.\n","21         @w_n_w              Can't seem to shake off this sadness.\n","22         @w_n_w            Feeling energized and ready to conquer.\n","23         @w_n_w           This gloomy weather is bringing me down.\n","24         @w_n_w                        Feeling really happy today.\n","25         @w_n_w                              I'm feeling so alone.\n","26         @w_n_w               Excited about the new opportunities.\n","27         @w_n_w                     Feeling hopeless and defeated.\n","28         @w_n_w                       Enjoying a peaceful evening.\n","29         @w_n_w                Feeling frustrated and overwhelmed.\n","30         @w_n_w                  Having a great time with friends.\n","31        @fatima                               going to sleep happy\n","32        @fatima                       #INDIACOREY blessed this day\n","33        @fatima                                they are the cutest\n","34        @fatima  when we have no contents/videos and we go look...\n","35        @fatima            netflix queue better post anything asap\n","36        @fatima             content always comes out when i go out\n","37        @fatima                            she is so funny i can’t\n","38        @fatima                                 he looks so gooood\n","39        @fatima                            this cast is everything\n","40        @fatima    btw i knew she was going to make a post my babe\n","41        @fatima               i love how we all have the same icon"],"text/html":["\n","  <div id=\"df-83013568-fdf5-4d4e-8351-f8120e10e33b\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>username</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>@isadia_</td>\n","      <td>Just had the best day ever!</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>@isadia_</td>\n","      <td>Feeling down today.</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>@isadia_</td>\n","      <td>So excited for the weekend!</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>@isadia_</td>\n","      <td>Can't believe how much I accomplished!</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>@isadia_</td>\n","      <td>This weather is awful.</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>@isadia_</td>\n","      <td>Feeling stressed out.</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>@isadia_</td>\n","      <td>Enjoying a relaxing evening.</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>@mishi_ali209</td>\n","      <td>Had a great time with friends.</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>@mishi_ali209</td>\n","      <td>Feeling sad and lonely.</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>@mishi_ali209</td>\n","      <td>Feeling down today.</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>@mishi_ali209</td>\n","      <td>Can't stop crying.</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>@mishi_ali209</td>\n","      <td>So happy to see my family.</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>@mishi_ali209</td>\n","      <td>I feel like everything is falling apart.</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>@mishi_ali209</td>\n","      <td>Enjoyed a nice walk in the park.</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>@mishi_ali209</td>\n","      <td>Feeling disappointed.</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>@mishi_ali209</td>\n","      <td>Looking forward to better days.</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>@mishi_ali209</td>\n","      <td>Feeling exhausted and drained.</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>@mishi_ali209</td>\n","      <td>Missing someone special.</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>@mishi_ali209</td>\n","      <td>Just had a great workout.</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>@w_n_w</td>\n","      <td>Had an amazing day!</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>@w_n_w</td>\n","      <td>Feeling positive and motivated.</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>@w_n_w</td>\n","      <td>Can't seem to shake off this sadness.</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>@w_n_w</td>\n","      <td>Feeling energized and ready to conquer.</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>@w_n_w</td>\n","      <td>This gloomy weather is bringing me down.</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>@w_n_w</td>\n","      <td>Feeling really happy today.</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>@w_n_w</td>\n","      <td>I'm feeling so alone.</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>@w_n_w</td>\n","      <td>Excited about the new opportunities.</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>@w_n_w</td>\n","      <td>Feeling hopeless and defeated.</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>@w_n_w</td>\n","      <td>Enjoying a peaceful evening.</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>@w_n_w</td>\n","      <td>Feeling frustrated and overwhelmed.</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>@w_n_w</td>\n","      <td>Having a great time with friends.</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>@fatima</td>\n","      <td>going to sleep happy</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>@fatima</td>\n","      <td>#INDIACOREY blessed this day</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>@fatima</td>\n","      <td>they are the cutest</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>@fatima</td>\n","      <td>when we have no contents/videos and we go look...</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>@fatima</td>\n","      <td>netflix queue better post anything asap</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>@fatima</td>\n","      <td>content always comes out when i go out</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>@fatima</td>\n","      <td>she is so funny i can’t</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>@fatima</td>\n","      <td>he looks so gooood</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>@fatima</td>\n","      <td>this cast is everything</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>@fatima</td>\n","      <td>btw i knew she was going to make a post my babe</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>@fatima</td>\n","      <td>i love how we all have the same icon</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-83013568-fdf5-4d4e-8351-f8120e10e33b')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-83013568-fdf5-4d4e-8351-f8120e10e33b button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-83013568-fdf5-4d4e-8351-f8120e10e33b');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["# Get user input for the username\n","#username = input(\"Enter the username to search: \")\n","username = input(\"Enter the username to search: \")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gr4eGbGCQqW_","outputId":"f54314a2-2162-4a32-ea84-203392d49258","executionInfo":{"status":"ok","timestamp":1685799961634,"user_tz":420,"elapsed":5049,"user":{"displayName":"Sadia Iqbal","userId":"13577405437173025394"}}},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter the username to search: @w_n_w\n"]}]},{"cell_type":"code","source":["# Filter the dataset based on the username\n","#tweets_df = df[df['username'] == username]\n","tweets_df = df[df['username'] == username]"],"metadata":{"id":"TGYK-bvLR81P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if tweets_df.empty:\n","    print(\"Username has not been found.\")\n","elif tweets_df.shape[0] < 10:\n","    print(\"Your data is not enough for analysis.\")\n","else:\n","#step 2: Preprocessing\n","\n","    def preprocess_text(df, column_name):\n","        # Convert to lowercase\n","        df[column_name] = df[column_name].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n","        # Removing numerical values\n","        df[column_name] = df[column_name].str.replace(\"\\d\", \"\")\n","        # Removing punctuations\n","        df[column_name] = df[column_name].str.replace(\"[^\\w\\s]\", \"\")\n","        df[column_name] = df[column_name].str.replace(r\"(\\x23.* )+\", \"\")\n","        df[column_name] = df[column_name].str.replace('_', '')\n","        df[column_name] = df[column_name].str.replace('__', '')\n","        # Removing double space\n","        df[column_name] = df[column_name].str.replace(\"\\s+\", \" \")\n","        # Removing user\n","        df[column_name] = df[column_name].str.replace('(@[A-Za-z]+[A-Za-z0-9-_]+)', '') # remove twitted at\n","        # Removing links\n","        df[column_name] = df[column_name].str.replace('http\\S+', '')\n","        # Removing small words which are less than given condition\n","        df[column_name] = df[column_name].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n","        # STOPWORDS\n","        sw = stopwords.words(\"english\")\n","        df[column_name] = df[column_name].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))\n","        # Lemmatization (forgot converted into forget)\n","        df[column_name] = df[column_name].apply(lambda x: \" \".join([Word(x).lemmatize()]))\n","        df[column_name+\"_tokens\"] = df[column_name].apply(lambda x: TextBlob(x).words)\n","        # Frequency Analysis\n","        df[column_name+\"_frequency\"] = df[column_name].apply(lambda x: len(str(x).split(\" \")))\n","\n","        return df\n","    # apply pre-processing function on'text'\n","    tweets_df = preprocess_text(tweets_df,'text')\n","#......................................................................................................................................\n","#step 3: Polarization\n","    positive_words_tweet = []\n","    negative_words_tweet = []\n","    neutral_words_tweet = []\n","\n","    positive_sentiment_score = 0\n","    negative_sentiment_score = 0\n","    neutral_sentiment_score = 0\n","\n","    N = []\n","    P = []\n","\n","\n","\n","    for index, row in tweets_df.iterrows():\n","        tweet = row['text']\n","\n","\n","        # Sentiment analysis for tweet_text column\n","        scores_tweet = sia.polarity_scores(tweet)\n","        tweets_df.at[index, 'tweet_positive_score'] = scores_tweet['pos']\n","        tweets_df.at[index, 'tweet_negative_score'] = scores_tweet['neg']\n","        tweets_df.at[index, 'tweet_neutral_score'] = scores_tweet['neu']\n","\n","        if scores_tweet['compound'] > 0.1:\n","            tweets_df.at[index, 'tweet_sentiment'] = 'positive'\n","            positive_sentiment_score += 1\n","            positive_words_tweet.extend(tweet.split())\n","            P.append(scores_tweet['pos'])  # Store positive score in P\n","\n","        elif scores_tweet['compound'] < -0.1:\n","            tweets_df.at[index, 'tweet_sentiment'] = 'negative'\n","            negative_sentiment_score += 1\n","            negative_words_tweet.extend(tweet.split())\n","            N.append(scores_tweet['neg'])  # Store negative score in N\n","\n","        else:\n","            tweets_df.at[index, 'tweet_sentiment'] = 'neutral'\n","            neutral_sentiment_score += 1\n","            neutral_words_tweet.extend(tweet.split())\n","    tweets_df.head(10)\n","\n","    # Filter the tweets_df dataframe to contain only the negative tweets and positive tweets\n","    negative_df = tweets_df.loc[tweets_df['tweet_sentiment'] == 'negative', ['text', 'tweet_negative_score']]\n","    positive_df = tweets_df.loc[tweets_df['tweet_sentiment'] == 'positive', ['text', 'tweet_positive_score']]\n","    #Step 4: apply saved model\n","    # Check if there are any negative tweets\n","\n","    neg_result = 0  # Initialize pos_result to 0\n","    if len(N) > 0:\n","        # Load the saved K-means labels\n","        with open('hierarchical_negative_labels.pkl', 'rb') as f:\n","            labels = pickle.load(f)\n","\n","\n","        # Assuming you have new testing data stored in a variable called `new_data`\n","\n","        # Apply the labels to the new testing data\n","        new_labels = labels[:tweets_df.shape[0]]  # Get labels for the same number of tweets as new data\n","\n","        # Filter out the cluster labels for the negative tweets\n","        negative_labels = new_labels[tweets_df['tweet_sentiment'] == 'negative']\n","\n","        # Print the cluster labels for the negative tweets\n","        for label in negative_labels:\n","            print(\"Cluster Label:\", label)\n","        # Assign descriptive names to the cluster labels\n","        cluster_names = {\n","            0: \"Highly Depressed\",\n","            1: \"Lightly Depressed\"\n","        }\n","\n","        # Count the number of tweets in each cluster\n","        cluster_counts = Counter(negative_labels)\n","\n","        # Get the cluster label with the maximum number of tweets\n","        max_cluster_label = max(cluster_counts, key=cluster_counts.get)\n","\n","        # Print the cluster labels and their corresponding counts\n","        for label, count in cluster_counts.items():\n","            cluster_name = cluster_names.get(label, \"Unknown\")\n","            print(f\"Cluster {label} ({cluster_name}) has {count} tweets.\")\n","\n","        # Print the cluster with the highest number of tweets\n","        max_cluster_name = cluster_names.get(max_cluster_label, \"Unknown\")\n","        print(f\"The cluster with the highest number of tweets is Cluster {max_cluster_label} ({max_cluster_name}).\")\n","        # Store the maximum cluster label in the 'result' variable\n","        neg_result = max_cluster_label\n","        print(neg_result)\n","        # Update the 'result' variable based on the value of 'max_cluster_label'\n","        if max_cluster_label == 0:\n","            neg_result = -2\n","        elif max_cluster_label == 1:\n","            neg_result = -1\n","\n","        print(neg_result)\n","\n","  #/////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n","    pos_result = 0  # Initialize pos_result to 0\n","    if len(P) > 0:\n","\n","        # Load the saved K-means labels\n","        with open('hierarchical_positive_labels.pkl', 'rb') as f:\n","            labels = pickle.load(f)\n","\n","        # Assuming you have new testing data stored in a variable called `new_data`\n","\n","        # Apply the labels to the new testing data\n","        new_labels = labels[:tweets_df.shape[0]]  # Get labels for the same number of tweets as new data\n","\n","        # Filter out the cluster labels for the negative tweets\n","        positive_labels = new_labels[tweets_df['tweet_sentiment'] == 'positive']\n","\n","        # Print the cluster labels for the negative tweets\n","        for label in positive_labels:\n","            print(\"Cluster Label:\", label)\n","        # Assign descriptive names to the cluster labels\n","        cluster_names = {\n","            0: \"lightly Positive\",\n","            1: \"Highly Positive\"\n","        }\n","\n","        # Count the number of tweets in each cluster\n","        cluster_counts = Counter(positive_labels)\n","\n","        # Get the cluster label with the maximum number of tweets\n","        max_cluster_label = max(cluster_counts, key=cluster_counts.get)\n","\n","        # Print the cluster labels and their corresponding counts\n","        for label, count in cluster_counts.items():\n","            cluster_name = cluster_names.get(label, \"Unknown\")\n","            #print(f\"Cluster {label} ({cluster_name}) has {count} tweets.\")\n","\n","        # Print the cluster with the highest number of tweets\n","        max_cluster_name = cluster_names.get(max_cluster_label, \"Unknown\")\n","        print(f\"The cluster with the highest number of tweets is Cluster {max_cluster_label} ({max_cluster_name}).\")\n","        # Store the maximum cluster label in the 'result' variable\n","        pos_result = max_cluster_label\n","        print(pos_result)\n","        # Update the 'result' variable based on the value of 'max_cluster_label'\n","        if max_cluster_label == 0:\n","            pos_result = 1\n","        elif max_cluster_label == 1:\n","            pos_result = 2\n","\n","        print(pos_result)\n","#/////////////////////////////////////////////////////////////////////////////////////////////////////\n","    # Count the total number of tweets\n","    total_tweets = len(tweets_df)\n","\n","    # Count the number of negative tweets\n","    negative_tweets = len(tweets_df[tweets_df['tweet_sentiment'] == 'negative'])\n","\n","    # Count the number of positive tweets\n","    positive_tweets = len(tweets_df[tweets_df['tweet_sentiment'] == 'positive'])\n","\n","    # Calculate the weights based on the counts\n","    negative_weight = negative_tweets / total_tweets\n","    positive_weight = positive_tweets / total_tweets\n","\n","    # Calculate the weighted average\n","    weighted_average = (negative_weight * neg_result) + (positive_weight * pos_result)\n","    print(\"Weighted Average:\", weighted_average)\n","\n","\n","    # Threshold ranges for negative sentiments\n","    negative_thresholds = {\n","        \"Lightly Depressed\": [-1.5, 0],\n","        \"Highly Depressed\": [-float('inf'), -1.5]\n","    }\n","\n","    # Threshold ranges for positive sentiments\n","    positive_thresholds = {\n","        \"Lightly Positive\": [0, 1.5],\n","        \"Highly Positive\": [1.5, float('inf')]\n","    }\n","\n","    # Categorize the weighted average for negative sentiments\n","    negative_category = None\n","    for category, threshold in negative_thresholds.items():\n","        if threshold[0] <= weighted_average <= threshold[1]:\n","            negative_category = category\n","            break\n","    if weighted_average == 0.0:\n","        negative_category = \"Lightly Depressed\"\n","\n","    # Categorize the weighted average for positive sentiments\n","    positive_category = None\n","    for category, threshold in positive_thresholds.items():\n","        if threshold[0] <= weighted_average <= threshold[1]:\n","            positive_category = category\n","            break\n","\n","    # Print the categorized weighted average\n","    if negative_category is not None:\n","        print(\"Weighted Average (Negative):\", weighted_average, \"Category:\", negative_category)\n","\n","    if positive_category is not None and weighted_average != 0.0:\n","        print(\"Weighted Average (Positive):\", weighted_average, \"Category:\", positive_category)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nCNKGgWqbEjR","executionInfo":{"status":"ok","timestamp":1685799961640,"user_tz":420,"elapsed":98,"user":{"displayName":"Sadia Iqbal","userId":"13577405437173025394"}},"outputId":"5dc60812-e9b7-4081-a94a-07a6380093e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cluster Label: 1\n","Cluster Label: 1\n","Cluster Label: 1\n","Cluster Label: 0\n","Cluster 1 (Highly Depressed) has 3 tweets.\n","Cluster 0 (Lightly Depressed) has 1 tweets.\n","The cluster with the highest number of tweets is Cluster 1 (Highly Depressed).\n","1\n","-2\n","Cluster Label: 0\n","Cluster Label: 0\n","Cluster Label: 0\n","Cluster Label: 0\n","Cluster Label: 0\n","Cluster Label: 0\n","Cluster Label: 0\n","Cluster Label: 0\n","The cluster with the highest number of tweets is Cluster 0 (lightly Positive).\n","0\n","1\n","Weighted Average: 0.0\n","Weighted Average (Negative): 0.0 Category: Lightly Depressed\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-29-72d80c4fee66>:10: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df[column_name] = df[column_name].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n","<ipython-input-29-72d80c4fee66>:12: FutureWarning: The default value of regex will change from True to False in a future version.\n","  df[column_name] = df[column_name].str.replace(\"\\d\", \"\")\n","<ipython-input-29-72d80c4fee66>:12: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df[column_name] = df[column_name].str.replace(\"\\d\", \"\")\n","<ipython-input-29-72d80c4fee66>:14: FutureWarning: The default value of regex will change from True to False in a future version.\n","  df[column_name] = df[column_name].str.replace(\"[^\\w\\s]\", \"\")\n","<ipython-input-29-72d80c4fee66>:14: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df[column_name] = df[column_name].str.replace(\"[^\\w\\s]\", \"\")\n","<ipython-input-29-72d80c4fee66>:15: FutureWarning: The default value of regex will change from True to False in a future version.\n","  df[column_name] = df[column_name].str.replace(r\"(\\x23.* )+\", \"\")\n","<ipython-input-29-72d80c4fee66>:15: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df[column_name] = df[column_name].str.replace(r\"(\\x23.* )+\", \"\")\n","<ipython-input-29-72d80c4fee66>:16: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df[column_name] = df[column_name].str.replace('_', '')\n","<ipython-input-29-72d80c4fee66>:17: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df[column_name] = df[column_name].str.replace('__', '')\n","<ipython-input-29-72d80c4fee66>:19: FutureWarning: The default value of regex will change from True to False in a future version.\n","  df[column_name] = df[column_name].str.replace(\"\\s+\", \" \")\n","<ipython-input-29-72d80c4fee66>:19: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df[column_name] = df[column_name].str.replace(\"\\s+\", \" \")\n","<ipython-input-29-72d80c4fee66>:21: FutureWarning: The default value of regex will change from True to False in a future version.\n","  df[column_name] = df[column_name].str.replace('(@[A-Za-z]+[A-Za-z0-9-_]+)', '') # remove twitted at\n","<ipython-input-29-72d80c4fee66>:21: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df[column_name] = df[column_name].str.replace('(@[A-Za-z]+[A-Za-z0-9-_]+)', '') # remove twitted at\n","<ipython-input-29-72d80c4fee66>:23: FutureWarning: The default value of regex will change from True to False in a future version.\n","  df[column_name] = df[column_name].str.replace('http\\S+', '')\n","<ipython-input-29-72d80c4fee66>:23: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df[column_name] = df[column_name].str.replace('http\\S+', '')\n","<ipython-input-29-72d80c4fee66>:25: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df[column_name] = df[column_name].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n","<ipython-input-29-72d80c4fee66>:28: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df[column_name] = df[column_name].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))\n","<ipython-input-29-72d80c4fee66>:30: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df[column_name] = df[column_name].apply(lambda x: \" \".join([Word(x).lemmatize()]))\n","<ipython-input-29-72d80c4fee66>:31: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df[column_name+\"_tokens\"] = df[column_name].apply(lambda x: TextBlob(x).words)\n","<ipython-input-29-72d80c4fee66>:33: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df[column_name+\"_frequency\"] = df[column_name].apply(lambda x: len(str(x).split(\" \")))\n","<ipython-input-29-72d80c4fee66>:59: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  tweets_df.at[index, 'tweet_positive_score'] = scores_tweet['pos']\n","<ipython-input-29-72d80c4fee66>:60: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  tweets_df.at[index, 'tweet_negative_score'] = scores_tweet['neg']\n","<ipython-input-29-72d80c4fee66>:61: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  tweets_df.at[index, 'tweet_neutral_score'] = scores_tweet['neu']\n","<ipython-input-29-72d80c4fee66>:64: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  tweets_df.at[index, 'tweet_sentiment'] = 'positive'\n"]}]},{"cell_type":"code","source":["\"\"\"\n","if tweets_df.empty:\n","    print(\"Username has not been found.\")\n","elif tweets_df.shape[0] < 10:\n","    print(\"Your data is not enough for analysis.\")\n","else:\n","#step 2: Preprocessing\n","\n","    def preprocess_text(df, column_name):\n","        # Convert to lowercase\n","        df[column_name] = df[column_name].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n","        # Removing numerical values\n","        df[column_name] = df[column_name].str.replace(\"\\d\", \"\")\n","        # Removing punctuations\n","        df[column_name] = df[column_name].str.replace(\"[^\\w\\s]\", \"\")\n","        df[column_name] = df[column_name].str.replace(r\"(\\x23.* )+\", \"\")\n","        df[column_name] = df[column_name].str.replace('_', '')\n","        df[column_name] = df[column_name].str.replace('__', '')\n","        # Removing double space\n","        df[column_name] = df[column_name].str.replace(\"\\s+\", \" \")\n","        # Removing user\n","        df[column_name] = df[column_name].str.replace('(@[A-Za-z]+[A-Za-z0-9-_]+)', '') # remove twitted at\n","        # Removing links\n","        df[column_name] = df[column_name].str.replace('http\\S+', '')\n","        # Removing small words which are less than given condition\n","        df[column_name] = df[column_name].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n","        # STOPWORDS\n","        sw = stopwords.words(\"english\")\n","        df[column_name] = df[column_name].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))\n","        # Lemmatization (forgot converted into forget)\n","        df[column_name] = df[column_name].apply(lambda x: \" \".join([Word(x).lemmatize()]))\n","        df[column_name+\"_tokens\"] = df[column_name].apply(lambda x: TextBlob(x).words)\n","        # Frequency Analysis\n","        df[column_name+\"_frequency\"] = df[column_name].apply(lambda x: len(str(x).split(\" \")))\n","\n","        return df\n","    # apply pre-processing function on'text'\n","    tweets_df = preprocess_text(tweets_df,'text')\n","#......................................................................................................................................\n","#step 3: Polarization\n","    positive_words_tweet = []\n","    negative_words_tweet = []\n","    neutral_words_tweet = []\n","\n","    positive_sentiment_score = 0\n","    negative_sentiment_score = 0\n","    neutral_sentiment_score = 0\n","\n","    N = []\n","    P = []\n","\n","\n","\n","    for index, row in tweets_df.iterrows():\n","        tweet = row['text']\n","\n","\n","        # Sentiment analysis for tweet_text column\n","        scores_tweet = sia.polarity_scores(tweet)\n","        tweets_df.at[index, 'tweet_positive_score'] = scores_tweet['pos']\n","        tweets_df.at[index, 'tweet_negative_score'] = scores_tweet['neg']\n","        tweets_df.at[index, 'tweet_neutral_score'] = scores_tweet['neu']\n","\n","        if scores_tweet['compound'] > 0.1:\n","            tweets_df.at[index, 'tweet_sentiment'] = 'positive'\n","            positive_sentiment_score += 1\n","            positive_words_tweet.extend(tweet.split())\n","            P.append(scores_tweet['pos'])  # Store positive score in P\n","\n","        elif scores_tweet['compound'] < -0.1:\n","            tweets_df.at[index, 'tweet_sentiment'] = 'negative'\n","            negative_sentiment_score += 1\n","            negative_words_tweet.extend(tweet.split())\n","            N.append(scores_tweet['neg'])  # Store negative score in N\n","\n","        else:\n","            tweets_df.at[index, 'tweet_sentiment'] = 'neutral'\n","            neutral_sentiment_score += 1\n","            neutral_words_tweet.extend(tweet.split())\n","    tweets_df.head(10)\n","\n","    # Filter the tweets_df dataframe to contain only the negative tweets and positive tweets\n","    negative_df = tweets_df.loc[tweets_df['tweet_sentiment'] == 'negative', ['text', 'tweet_negative_score']]\n","    positive_df = tweets_df.loc[tweets_df['tweet_sentiment'] == 'positive', ['text', 'tweet_positive_score']]\n","    #Step 4: apply saved model\n","    # Check if there are any negative tweets\n","\n","    if len(N) > 0:\n","        # Load the saved K-means labels\n","        with open('hierarchical_negative_labels.pkl', 'rb') as f:\n","            labels = pickle.load(f)\n","\n","\n","        # Assuming you have new testing data stored in a variable called `new_data`\n","\n","        # Apply the labels to the new testing data\n","        new_labels = labels[:tweets_df.shape[0]]  # Get labels for the same number of tweets as new data\n","\n","        # Filter out the cluster labels for the negative tweets\n","        negative_labels = new_labels[tweets_df['tweet_sentiment'] == 'negative']\n","\n","        # Print the cluster labels for the negative tweets\n","        for label in negative_labels:\n","            print(\"Cluster Label:\", label)\n","        # Assign descriptive names to the cluster labels\n","        cluster_names = {\n","            0: \"Lightly Depressed\",\n","            1: \"Highly Depressed\",\n","        }\n","\n","        # Count the number of tweets in each cluster\n","        cluster_counts = Counter(negative_labels)\n","\n","        # Get the cluster label with the maximum number of tweets\n","        max_cluster_label = max(cluster_counts, key=cluster_counts.get)\n","\n","        # Print the cluster labels and their corresponding counts\n","        for label, count in cluster_counts.items():\n","            cluster_name = cluster_names.get(label, \"Unknown\")\n","            print(f\"Cluster {label} ({cluster_name}) has {count} tweets.\")\n","\n","        # Print the cluster with the highest number of tweets\n","        max_cluster_name = cluster_names.get(max_cluster_label, \"Unknown\")\n","        print(f\"The cluster with the highest number of tweets is Cluster {max_cluster_label} ({max_cluster_name}).\")\n","\n","\n","        # Display negative words used in the respective cluster\n","        if max_cluster_label == 0:  # Lightly Depressed cluster\n","           negative_tweets_lightly = tweets_df[(tweets_df['tweet_sentiment'] == 'negative') & (new_labels == max_cluster_label)]\n","           unique_negative_words_lightly = set(negative_tweets_lightly['text'].apply(lambda tweet: tweet.split()).sum())\n","           print(\"Negative words used only in tweets of Lightly Depressed cluster:\")\n","           print(unique_negative_words_lightly)\n","        elif max_cluster_label == 1:  # Highly Depressed cluster\n","           negative_tweets_highly = tweets_df[(tweets_df['tweet_sentiment'] == 'negative') & (new_labels == max_cluster_label)]\n","           unique_negative_words_highly = set(negative_tweets_highly['text'].apply(lambda tweet: tweet.split()).sum())\n","           print(\"Negative words used only in tweets of Highly Depressed cluster:\")\n","           print(unique_negative_words_highly)\n","           print(unique_negative_words_highly)\n","    else:\n","\n","        # Load the saved K-means labels\n","        with open('hierarchical_positive_labels.pkl', 'rb') as f:\n","            labels = pickle.load(f)\n","\n","        # Assuming you have new testing data stored in a variable called `new_data`\n","\n","        # Apply the labels to the new testing data\n","        new_labels = labels[:tweets_df.shape[0]]  # Get labels for the same number of tweets as new data\n","\n","        # Filter out the cluster labels for the negative tweets\n","        positive_labels = new_labels[tweets_df['tweet_sentiment'] == 'positive']\n","\n","        # Print the cluster labels for the negative tweets\n","        for label in positive_labels:\n","            print(\"Cluster Label:\", label)\n","        # Assign descriptive names to the cluster labels\n","        cluster_names = {\n","            0: \"lightly Positive\",\n","            1: \"Highly Depressed\",\n","        }\n","\n","        # Count the number of tweets in each cluster\n","        cluster_counts = Counter(positive_labels)\n","\n","        # Get the cluster label with the maximum number of tweets\n","        max_cluster_label = max(cluster_counts, key=cluster_counts.get)\n","\n","        # Print the cluster labels and their corresponding counts\n","        for label, count in cluster_counts.items():\n","            cluster_name = cluster_names.get(label, \"Unknown\")\n","            print(f\"Cluster {label} ({cluster_name}) has {count} tweets.\")\n","        results = [] #list where i want to keep print statements data\n","        # Print the cluster with the highest number of tweets\n","        max_cluster_name = cluster_names.get(max_cluster_label, \"Unknown\")\n","        print(f\"The cluster with the highest number of tweets is Cluster {max_cluster_label} ({max_cluster_name}).\")\n","        results.append(f\"The cluster with the highest number of tweets is Cluster {max_cluster_label} ({max_cluster_name}).\")\n","        # Display negative words used in the respective cluster\n","        if max_cluster_label == 0:  # Lightly Positive cluster\n","           positive_tweets_lightly = tweets_df[(tweets_df['tweet_sentiment'] == 'positive') & (new_labels == max_cluster_label)]\n","           unique_positive_words_lightly = set(positive_tweets_lightly['text'].apply(lambda tweet: tweet.split()).sum())\n","           print(\"Positive words used only in tweets of Lightly Positive cluster:\")\n","           print(unique_positive_words_lightly)\n","           results.append(\"Positive words used only in tweets of Lightly Positive cluster:\")\n","           results.append(unique_positive_words_lightly)\n","        elif max_cluster_label == 1:  # Highly Positive cluster\n","           positive_tweets_highly = tweets_df[(tweets_df['tweet_sentiment'] == 'positive') & (new_labels == max_cluster_label)]\n","           unique_positive_words_highly = set(positive_tweets_highly['text'].apply(lambda tweet: tweet.split()).sum())\n","           print(\"Positive words used only in tweets of Highly Positive cluster:\")\n","           print(unique_positive_words_highly)\n","           results.append(\"Positive words used only in tweets of Highly Depressed cluster:\")\n","           results.append(unique_positive_words_highly)\n","\n","  \"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":128},"id":"DxB6L47LSFkF","outputId":"a49b2f75-bb3b-48f1-990b-f9995a96f38c","executionInfo":{"status":"ok","timestamp":1685799961642,"user_tz":420,"elapsed":41,"user":{"displayName":"Sadia Iqbal","userId":"13577405437173025394"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nif tweets_df.empty:\\n    print(\"Username has not been found.\")\\nelif tweets_df.shape[0] < 10:\\n    print(\"Your data is not enough for analysis.\")\\nelse:\\n#step 2: Preprocessing  \\n      \\n    def preprocess_text(df, column_name):\\n        # Convert to lowercase\\n        df[column_name] = df[column_name].apply(lambda x: \" \".join(x.lower() for x in x.split()))\\n        # Removing numerical values\\n        df[column_name] = df[column_name].str.replace(\"\\\\d\", \"\")\\n        # Removing punctuations\\n        df[column_name] = df[column_name].str.replace(\"[^\\\\w\\\\s]\", \"\")\\n        df[column_name] = df[column_name].str.replace(r\"(#.* )+\", \"\")\\n        df[column_name] = df[column_name].str.replace(\\'_\\', \\'\\')\\n        df[column_name] = df[column_name].str.replace(\\'__\\', \\'\\')\\n        # Removing double space\\n        df[column_name] = df[column_name].str.replace(\"\\\\s+\", \" \")\\n        # Removing user\\n        df[column_name] = df[column_name].str.replace(\\'(@[A-Za-z]+[A-Za-z0-9-_]+)\\', \\'\\') # remove twitted at\\n        # Removing links\\n        df[column_name] = df[column_name].str.replace(\\'http\\\\S+\\', \\'\\')\\n        # Removing small words which are less than given condition\\n        df[column_name] = df[column_name].apply(lambda x: \\' \\'.join([w for w in x.split() if len(w)>3]))\\n        # STOPWORDS\\n        sw = stopwords.words(\"english\")\\n        df[column_name] = df[column_name].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))\\n        # Lemmatization (forgot converted into forget)\\n        df[column_name] = df[column_name].apply(lambda x: \" \".join([Word(x).lemmatize()]))\\n        df[column_name+\"_tokens\"] = df[column_name].apply(lambda x: TextBlob(x).words)\\n        # Frequency Analysis\\n        df[column_name+\"_frequency\"] = df[column_name].apply(lambda x: len(str(x).split(\" \")))\\n \\n        return df\\n    # apply pre-processing function on\\'text\\'\\n    tweets_df = preprocess_text(tweets_df,\\'text\\')\\n#......................................................................................................................................\\n#step 3: Polarization\\n    positive_words_tweet = []\\n    negative_words_tweet = []\\n    neutral_words_tweet = []\\n\\n    positive_sentiment_score = 0\\n    negative_sentiment_score = 0\\n    neutral_sentiment_score = 0\\n\\n    N = []\\n    P = []\\n\\n\\n\\n    for index, row in tweets_df.iterrows():\\n        tweet = row[\\'text\\']\\n   \\n    \\n        # Sentiment analysis for tweet_text column\\n        scores_tweet = sia.polarity_scores(tweet)\\n        tweets_df.at[index, \\'tweet_positive_score\\'] = scores_tweet[\\'pos\\']\\n        tweets_df.at[index, \\'tweet_negative_score\\'] = scores_tweet[\\'neg\\']\\n        tweets_df.at[index, \\'tweet_neutral_score\\'] = scores_tweet[\\'neu\\']\\n    \\n        if scores_tweet[\\'compound\\'] > 0.1:\\n            tweets_df.at[index, \\'tweet_sentiment\\'] = \\'positive\\'\\n            positive_sentiment_score += 1\\n            positive_words_tweet.extend(tweet.split())\\n            P.append(scores_tweet[\\'pos\\'])  # Store positive score in P\\n\\n        elif scores_tweet[\\'compound\\'] < -0.1:\\n            tweets_df.at[index, \\'tweet_sentiment\\'] = \\'negative\\'\\n            negative_sentiment_score += 1\\n            negative_words_tweet.extend(tweet.split())\\n            N.append(scores_tweet[\\'neg\\'])  # Store negative score in N\\n        \\n        else:\\n            tweets_df.at[index, \\'tweet_sentiment\\'] = \\'neutral\\'\\n            neutral_sentiment_score += 1\\n            neutral_words_tweet.extend(tweet.split())\\n    tweets_df.head(10)\\n    \\n    # Filter the tweets_df dataframe to contain only the negative tweets and positive tweets\\n    negative_df = tweets_df.loc[tweets_df[\\'tweet_sentiment\\'] == \\'negative\\', [\\'text\\', \\'tweet_negative_score\\']]\\n    positive_df = tweets_df.loc[tweets_df[\\'tweet_sentiment\\'] == \\'positive\\', [\\'text\\', \\'tweet_positive_score\\']] \\n    #Step 4: apply saved model\\n    # Check if there are any negative tweets\\n\\n    if len(N) > 0:\\n        # Load the saved K-means labels\\n        with open(\\'hierarchical_negative_labels.pkl\\', \\'rb\\') as f:\\n            labels = pickle.load(f)\\n\\n\\n        # Assuming you have new testing data stored in a variable called `new_data`\\n\\n        # Apply the labels to the new testing data\\n        new_labels = labels[:tweets_df.shape[0]]  # Get labels for the same number of tweets as new data\\n\\n        # Filter out the cluster labels for the negative tweets\\n        negative_labels = new_labels[tweets_df[\\'tweet_sentiment\\'] == \\'negative\\']\\n\\n        # Print the cluster labels for the negative tweets\\n        for label in negative_labels:\\n            print(\"Cluster Label:\", label)\\n        # Assign descriptive names to the cluster labels\\n        cluster_names = {\\n            0: \"Lightly Depressed\",\\n            1: \"Highly Depressed\",\\n        }\\n\\n        # Count the number of tweets in each cluster\\n        cluster_counts = Counter(negative_labels)\\n\\n        # Get the cluster label with the maximum number of tweets\\n        max_cluster_label = max(cluster_counts, key=cluster_counts.get)\\n\\n        # Print the cluster labels and their corresponding counts\\n        for label, count in cluster_counts.items():\\n            cluster_name = cluster_names.get(label, \"Unknown\")\\n            print(f\"Cluster {label} ({cluster_name}) has {count} tweets.\")\\n\\n        # Print the cluster with the highest number of tweets\\n        max_cluster_name = cluster_names.get(max_cluster_label, \"Unknown\")\\n        print(f\"The cluster with the highest number of tweets is Cluster {max_cluster_label} ({max_cluster_name}).\")\\n\\n        \\n        # Display negative words used in the respective cluster\\n        if max_cluster_label == 0:  # Lightly Depressed cluster\\n           negative_tweets_lightly = tweets_df[(tweets_df[\\'tweet_sentiment\\'] == \\'negative\\') & (new_labels == max_cluster_label)]\\n           unique_negative_words_lightly = set(negative_tweets_lightly[\\'text\\'].apply(lambda tweet: tweet.split()).sum())\\n           print(\"Negative words used only in tweets of Lightly Depressed cluster:\")\\n           print(unique_negative_words_lightly)\\n        elif max_cluster_label == 1:  # Highly Depressed cluster\\n           negative_tweets_highly = tweets_df[(tweets_df[\\'tweet_sentiment\\'] == \\'negative\\') & (new_labels == max_cluster_label)]\\n           unique_negative_words_highly = set(negative_tweets_highly[\\'text\\'].apply(lambda tweet: tweet.split()).sum())\\n           print(\"Negative words used only in tweets of Highly Depressed cluster:\")\\n           print(unique_negative_words_highly)\\n           print(unique_negative_words_highly)\\n    else:\\n\\n        # Load the saved K-means labels\\n        with open(\\'hierarchical_positive_labels.pkl\\', \\'rb\\') as f:\\n            labels = pickle.load(f)\\n\\n        # Assuming you have new testing data stored in a variable called `new_data`\\n\\n        # Apply the labels to the new testing data\\n        new_labels = labels[:tweets_df.shape[0]]  # Get labels for the same number of tweets as new data\\n\\n        # Filter out the cluster labels for the negative tweets\\n        positive_labels = new_labels[tweets_df[\\'tweet_sentiment\\'] == \\'positive\\']\\n\\n        # Print the cluster labels for the negative tweets\\n        for label in positive_labels:\\n            print(\"Cluster Label:\", label)\\n        # Assign descriptive names to the cluster labels\\n        cluster_names = {\\n            0: \"lightly Positive\",\\n            1: \"Highly Depressed\",\\n        }\\n\\n        # Count the number of tweets in each cluster\\n        cluster_counts = Counter(positive_labels)\\n\\n        # Get the cluster label with the maximum number of tweets\\n        max_cluster_label = max(cluster_counts, key=cluster_counts.get)\\n\\n        # Print the cluster labels and their corresponding counts\\n        for label, count in cluster_counts.items():\\n            cluster_name = cluster_names.get(label, \"Unknown\")\\n            print(f\"Cluster {label} ({cluster_name}) has {count} tweets.\")\\n        results = [] #list where i want to keep print statements data\\n        # Print the cluster with the highest number of tweets\\n        max_cluster_name = cluster_names.get(max_cluster_label, \"Unknown\")\\n        print(f\"The cluster with the highest number of tweets is Cluster {max_cluster_label} ({max_cluster_name}).\")\\n        results.append(f\"The cluster with the highest number of tweets is Cluster {max_cluster_label} ({max_cluster_name}).\")\\n        # Display negative words used in the respective cluster\\n        if max_cluster_label == 0:  # Lightly Positive cluster\\n           positive_tweets_lightly = tweets_df[(tweets_df[\\'tweet_sentiment\\'] == \\'positive\\') & (new_labels == max_cluster_label)]\\n           unique_positive_words_lightly = set(positive_tweets_lightly[\\'text\\'].apply(lambda tweet: tweet.split()).sum())\\n           print(\"Positive words used only in tweets of Lightly Positive cluster:\")\\n           print(unique_positive_words_lightly)\\n           results.append(\"Positive words used only in tweets of Lightly Positive cluster:\")\\n           results.append(unique_positive_words_lightly)\\n        elif max_cluster_label == 1:  # Highly Positive cluster\\n           positive_tweets_highly = tweets_df[(tweets_df[\\'tweet_sentiment\\'] == \\'positive\\') & (new_labels == max_cluster_label)]\\n           unique_positive_words_highly = set(positive_tweets_highly[\\'text\\'].apply(lambda tweet: tweet.split()).sum())\\n           print(\"Positive words used only in tweets of Highly Positive cluster:\")\\n           print(unique_positive_words_highly)\\n           results.append(\"Positive words used only in tweets of Highly Depressed cluster:\")\\n           results.append(unique_positive_words_highly)\\n\\n  '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":30}]}]}